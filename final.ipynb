{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 3.101780981301624\n",
      "Epoch 2/50, Loss: 3.070237892668103\n",
      "Epoch 3/50, Loss: 3.0410785745979196\n",
      "Epoch 4/50, Loss: 3.0139881115115594\n",
      "Epoch 5/50, Loss: 2.9887099435893743\n",
      "Epoch 6/50, Loss: 2.965022850172187\n",
      "Epoch 7/50, Loss: 2.942739397965661\n",
      "Epoch 8/50, Loss: 2.921701695983401\n",
      "Epoch 9/50, Loss: 2.9017753837332325\n",
      "Epoch 10/50, Loss: 2.882835483290364\n",
      "Epoch 11/50, Loss: 2.8647814339150677\n",
      "Epoch 12/50, Loss: 2.84752813978086\n",
      "Epoch 13/50, Loss: 2.8309983348151766\n",
      "Epoch 14/50, Loss: 2.815122943483556\n",
      "Epoch 15/50, Loss: 2.79984457761786\n",
      "Epoch 16/50, Loss: 2.7851056965208616\n",
      "Epoch 17/50, Loss: 2.770865343749279\n",
      "Epoch 18/50, Loss: 2.7570832613457616\n",
      "Epoch 19/50, Loss: 2.743720992507786\n",
      "Epoch 20/50, Loss: 2.7307420688488\n",
      "Epoch 21/50, Loss: 2.718117621467126\n",
      "Epoch 22/50, Loss: 2.705823372609964\n",
      "Epoch 23/50, Loss: 2.69383065371097\n",
      "Epoch 24/50, Loss: 2.682119267444408\n",
      "Epoch 25/50, Loss: 2.6706679663958455\n",
      "Epoch 26/50, Loss: 2.659460033543948\n",
      "Epoch 27/50, Loss: 2.648479581156597\n",
      "Epoch 28/50, Loss: 2.6377089326358556\n",
      "Epoch 29/50, Loss: 2.6271379279971168\n",
      "Epoch 30/50, Loss: 2.6167537238088117\n",
      "Epoch 31/50, Loss: 2.606545039166931\n",
      "Epoch 32/50, Loss: 2.5965045976830137\n",
      "Epoch 33/50, Loss: 2.586618946664956\n",
      "Epoch 34/50, Loss: 2.5768781462806922\n",
      "Epoch 35/50, Loss: 2.567274703483381\n",
      "Epoch 36/50, Loss: 2.557800153676889\n",
      "Epoch 37/50, Loss: 2.5484531979486476\n",
      "Epoch 38/50, Loss: 2.5392268287603192\n",
      "Epoch 39/50, Loss: 2.5301128225218243\n",
      "Epoch 40/50, Loss: 2.5211043077687054\n",
      "Epoch 41/50, Loss: 2.5122000547032712\n",
      "Epoch 42/50, Loss: 2.503394078292856\n",
      "Epoch 43/50, Loss: 2.4946808417130386\n",
      "Epoch 44/50, Loss: 2.4860563729631724\n",
      "Epoch 45/50, Loss: 2.4775173751901627\n",
      "Epoch 46/50, Loss: 2.469061799628939\n",
      "Epoch 47/50, Loss: 2.4606854949244896\n",
      "Epoch 48/50, Loss: 2.4523846246753433\n",
      "Epoch 49/50, Loss: 2.4441555490894014\n",
      "Epoch 50/50, Loss: 2.4359971693270746\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Any, Tuple, Optional, Dict, Iterator, Union\n",
    "\n",
    "grad_tracking_enabled = True\n",
    "\n",
    "def wrap_forward_function(func):\n",
    "    def new_function(*args, **kwargs):\n",
    "        arguments = tuple([a for a in args])\n",
    "        result = func(*args, **kwargs)\n",
    "        requires_grad = grad_tracking_enabled and any([isinstance(a, Parameter) and a.requires_grad for a in args])\n",
    "        if requires_grad:\n",
    "            result.parents = arguments\n",
    "            result.func = new_function  \n",
    "            result.kwargs = kwargs\n",
    "            result.requires_grad = True\n",
    "        return result\n",
    "    new_function.__name__ = func.__name__\n",
    "    return new_function\n",
    "\n",
    "\n",
    "class Parameter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        array: Optional[np.ndarray] = None,\n",
    "        requires_grad: bool = False,\n",
    "        parents: Optional[Tuple['Parameter', ...]] = None,\n",
    "        func: Optional[Callable] = None,\n",
    "        kwargs: Optional[dict] = None,\n",
    "    ):\n",
    "        self.array = array if array is not None else np.array(0.0)\n",
    "        self.grad: Optional[np.ndarray] = None\n",
    "        self.requires_grad = requires_grad\n",
    "        self.parents = parents if parents is not None else ()\n",
    "        self.func = func\n",
    "        self.kwargs = kwargs if kwargs is not None else {}\n",
    "\n",
    "    # Adjusted methods to handle scalars\n",
    "    def __add__(self, other: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(other, Parameter):\n",
    "            other = Parameter(np.array(other), requires_grad=False)\n",
    "        return add(self, other)\n",
    "\n",
    "    def __radd__(self, other: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(other, Parameter):\n",
    "            other = Parameter(np.array(other), requires_grad=False)\n",
    "        return add(other, self)\n",
    "\n",
    "    def __sub__(self, other: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(other, Parameter):\n",
    "            other = Parameter(np.array(other), requires_grad=False)\n",
    "        return subtract(self, other)\n",
    "\n",
    "    def __rsub__(self, other: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(other, Parameter):\n",
    "            other = Parameter(np.array(other), requires_grad=False)\n",
    "        return subtract(other, self)\n",
    "\n",
    "    def __mul__(self, other: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(other, Parameter):\n",
    "            other = Parameter(np.array(other), requires_grad=False)\n",
    "        return multiply(self, other)\n",
    "\n",
    "    def __rmul__(self, other: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(other, Parameter):\n",
    "            other = Parameter(np.array(other), requires_grad=False)\n",
    "        return multiply(other, self)\n",
    "\n",
    "    def __truediv__(self, other: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(other, Parameter):\n",
    "            other = Parameter(np.array(other), requires_grad=False)\n",
    "        return divide_op(self, other)\n",
    "\n",
    "    def __rtruediv__(self, other: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(other, Parameter):\n",
    "            other = Parameter(np.array(other), requires_grad=False)\n",
    "        return divide_op(other, self)\n",
    "\n",
    "\n",
    "    def __pow__(self, power: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(power, Parameter):\n",
    "            power = Parameter(np.array(power), requires_grad=False)\n",
    "        return power_func(self, power)\n",
    "\n",
    "    def __neg__(self) -> 'Parameter':\n",
    "        return negate(self)\n",
    "\n",
    "    def __matmul__(self, other: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(other, Parameter):\n",
    "            raise ValueError(\"Cannot perform matmul with non-Parameter type\")\n",
    "        return matmul(self, other)\n",
    "\n",
    "    def __rmatmul__(self, other: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(other, Parameter):\n",
    "            raise ValueError(\"Cannot perform matmul with non-Parameter type\")\n",
    "        return matmul(other, self)\n",
    "\n",
    "    def exp(self) -> 'Parameter':\n",
    "        return exp(self)\n",
    "\n",
    "    def log(self) -> 'Parameter':\n",
    "        return log(self)\n",
    "\n",
    "    def sin(self) -> 'Parameter':\n",
    "        return sin(self)\n",
    "\n",
    "    def cos(self) -> 'Parameter':\n",
    "        return cos(self)\n",
    "\n",
    "    def tanh(self) -> 'Parameter':\n",
    "        return tanh(self)\n",
    "\n",
    "    def sigmoid(self) -> 'Parameter':\n",
    "        return sigmoid(self)\n",
    "\n",
    "    def relu(self) -> 'Parameter':\n",
    "        return relu(self)\n",
    "\n",
    "    def sqrt(self) -> 'Parameter':\n",
    "        return sqrt(self)\n",
    "\n",
    "    def abs(self) -> 'Parameter':\n",
    "        return abs_func(self)\n",
    "\n",
    "    def reshape(self, *shape) -> 'Parameter':\n",
    "        return reshape(self, shape)\n",
    "\n",
    "    def sum(self, axis=None, keepdims=False) -> 'Parameter':\n",
    "        return sum_func(self, axis=axis, keepdims=keepdims)\n",
    "\n",
    "    @property\n",
    "    def T(self) -> 'Parameter':\n",
    "        return transpose(self)\n",
    "\n",
    "    def backward(self, grad: Optional[np.ndarray] = None):\n",
    "        if not self.requires_grad:\n",
    "            return\n",
    "        if grad is None:\n",
    "            grad = np.ones_like(self.array)\n",
    "        if self.grad is None:\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad\n",
    "        topo_order = self._topological_sort()\n",
    "        for tensor in reversed(topo_order):\n",
    "            if tensor.func is None:\n",
    "                continue\n",
    "            for idx, parent in enumerate(tensor.parents):\n",
    "                backward_func = lookup.get_backward_function(tensor.func, idx)\n",
    "                parent_grad = backward_func(*tensor.parents, tensor.grad, **tensor.kwargs)\n",
    "                if parent.requires_grad and parent_grad is not None:\n",
    "                    if parent.grad is None:\n",
    "                        parent.grad = parent_grad\n",
    "                    else:\n",
    "                        parent.grad += parent_grad\n",
    "\n",
    "    def _topological_sort(self) -> list:\n",
    "        visited = set()\n",
    "        topo_order = []\n",
    "        def dfs(tensor: 'Parameter'):\n",
    "            if tensor in visited:\n",
    "                return\n",
    "            visited.add(tensor)\n",
    "            for parent in tensor.parents:\n",
    "                dfs(parent)\n",
    "            topo_order.append(tensor)\n",
    "        dfs(self)\n",
    "        return topo_order\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.grad = None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Parameter(shape={self.array.shape}, requires_grad={self.requires_grad})\"\n",
    "\n",
    "class BackwardLookupTable:\n",
    "    def __init__(self):\n",
    "        self.table: Dict[Callable, Dict[int, Callable]] = defaultdict(dict)\n",
    "\n",
    "    def add_element(self, forward_function: Callable, position: int, backward_func: Callable):\n",
    "        self.table[forward_function][position] = backward_func\n",
    "\n",
    "    def get_backward_function(self, forward_function: Callable, position: int) -> Callable:\n",
    "        if forward_function not in self.table or position not in self.table[forward_function]:\n",
    "            raise KeyError(f\"No backward function found for {forward_function} at position {position}\")\n",
    "        return self.table[forward_function][position]\n",
    "\n",
    "lookup = BackwardLookupTable()\n",
    "\n",
    "def _broadcast_backward(grad_out, target_shape):\n",
    "    grad = grad_out\n",
    "    while len(grad.shape) > len(target_shape):\n",
    "        grad = grad.sum(axis=0)\n",
    "    for axis, size in enumerate(target_shape):\n",
    "        if size == 1:\n",
    "            grad = grad.sum(axis=axis, keepdims=True)\n",
    "    return grad\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def add(x, y):\n",
    "    return Parameter(array=x.array + y.array)\n",
    "\n",
    "def add_back0(x, y, grad_out):\n",
    "    grad = _broadcast_backward(grad_out, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "def add_back1(x, y, grad_out):\n",
    "    grad = _broadcast_backward(grad_out, y.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(add, 0, add_back0)\n",
    "lookup.add_element(add, 1, add_back1)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def subtract(x, y):\n",
    "    return Parameter(array=x.array - y.array)\n",
    "\n",
    "def subtract_back0(x, y, grad_out):\n",
    "    grad = _broadcast_backward(grad_out, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "def subtract_back1(x, y, grad_out):\n",
    "    grad = -_broadcast_backward(grad_out, y.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(subtract, 0, subtract_back0)\n",
    "lookup.add_element(subtract, 1, subtract_back1)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def multiply(x, y):\n",
    "    return Parameter(array=x.array * y.array)\n",
    "\n",
    "def multiply_back0(x, y, grad_out):\n",
    "    grad = grad_out * y.array\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "def multiply_back1(x, y, grad_out):\n",
    "    grad = grad_out * x.array\n",
    "    grad = _broadcast_backward(grad, y.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(multiply, 0, multiply_back0)\n",
    "lookup.add_element(multiply, 1, multiply_back1)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def divide_op(x, y):\n",
    "    return Parameter(array=x.array / y.array)\n",
    "\n",
    "def divide_back0(x, y, grad_out):\n",
    "    grad = grad_out / y.array\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "def divide_back1(x, y, grad_out):\n",
    "    if y.requires_grad:\n",
    "        grad = -grad_out * x.array / (y.array ** 2 + 1e-12)\n",
    "        grad = _broadcast_backward(grad, y.array.shape)\n",
    "        return grad\n",
    "    else:\n",
    "        return None  \n",
    "\n",
    "lookup.add_element(divide_op, 0, divide_back0)\n",
    "lookup.add_element(divide_op, 1, divide_back1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def power_func(x, y):\n",
    "    return Parameter(array=x.array ** y.array)\n",
    "\n",
    "def power_back0(x, y, grad_out):\n",
    "    grad = grad_out * y.array * (x.array ** (y.array - 1))\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "def power_back1(x, y, grad_out):\n",
    "    grad = grad_out * (x.array ** y.array) * np.log(x.array + 1e-12)\n",
    "    grad = _broadcast_backward(grad, y.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(power_func, 0, power_back0)\n",
    "lookup.add_element(power_func, 1, power_back1)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def negate(x):\n",
    "    return Parameter(array=-x.array)\n",
    "\n",
    "def negate_back(x, grad_out):\n",
    "    grad = -grad_out\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(negate, 0, negate_back)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def matmul(x, y):\n",
    "    return Parameter(array=x.array @ y.array)\n",
    "\n",
    "def matmul_back0(x, y, grad_out):\n",
    "    grad = grad_out @ y.array.T\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "def matmul_back1(x, y, grad_out):\n",
    "    grad = x.array.T @ grad_out\n",
    "    grad = _broadcast_backward(grad, y.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(matmul, 0, matmul_back0)\n",
    "lookup.add_element(matmul, 1, matmul_back1)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def exp(x):\n",
    "    return Parameter(array=np.exp(x.array))\n",
    "\n",
    "def exp_back(x, grad_out):\n",
    "    grad = grad_out * np.exp(x.array)\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(exp, 0, exp_back)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def log(x):\n",
    "    return Parameter(array=np.log(x.array + 1e-12))\n",
    "\n",
    "def log_back(x, grad_out):\n",
    "    grad = grad_out / (x.array + 1e-12)\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(log, 0, log_back)\n",
    "\n",
    "@wrap_forward_function\n",
    "def sum_func(x, axis=None, keepdims=False):\n",
    "    return Parameter(array=np.sum(x.array, axis=axis, keepdims=keepdims))\n",
    "\n",
    "def sum_back(x, grad_out, axis=None, keepdims=False):\n",
    "    grad = np.broadcast_to(grad_out, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(sum_func, 0, sum_back)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def max_func(x, axis=None, keepdims=False):\n",
    "    return Parameter(array=np.max(x.array, axis=axis, keepdims=keepdims))\n",
    "\n",
    "def max_back(x, grad_out, axis=None, keepdims=False):\n",
    "    grad = np.zeros_like(x.array)\n",
    "    max_vals = np.max(x.array, axis=axis, keepdims=True)\n",
    "    mask = (x.array == max_vals)\n",
    "    grad += mask * grad_out\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "\n",
    "lookup.add_element(max_func, 0, max_back)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def transpose(x):\n",
    "    return Parameter(array=x.array.T)\n",
    "\n",
    "def transpose_back(x, grad_out):\n",
    "    return grad_out.T\n",
    "\n",
    "lookup.add_element(transpose, 0, transpose_back)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def relu(x):\n",
    "    return Parameter(array=np.maximum(0, x.array))\n",
    "\n",
    "def relu_back(x, grad_out):\n",
    "    grad = grad_out.copy()\n",
    "    grad[x.array <= 0] = 0\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(relu, 0, relu_back)\n",
    "\n",
    "def softmax(x):\n",
    "    x_max = max_func(x, axis=1, keepdims=True)\n",
    "    exp_x = exp(x - x_max)\n",
    "    sum_exp_x = sum_func(exp_x, axis=1, keepdims=True)\n",
    "    return exp_x / sum_exp_x\n",
    "\n",
    "\n",
    "def one_hot(targets, num_classes):\n",
    "    targets_array = targets.array.astype(int)\n",
    "    one_hot_array = np.zeros((len(targets_array), num_classes))\n",
    "    one_hot_array[np.arange(len(targets_array)), targets_array] = 1\n",
    "    return Parameter(one_hot_array, requires_grad=False)\n",
    "\n",
    "def cross_entropy_loss(predictions, targets):\n",
    "    probs = softmax(predictions)\n",
    "    N = predictions.array.shape[0]\n",
    "    target_probs = one_hot(targets, num_classes=predictions.array.shape[1])\n",
    "    epsilon = 1e-12\n",
    "    log_probs = log(probs + epsilon)\n",
    "    loss = -sum_func(target_probs * log_probs) / N\n",
    "    return loss\n",
    "\n",
    "\n",
    "class Module:\n",
    "    def __init__(self):\n",
    "        self._modules = {}\n",
    "        self._parameters = {}\n",
    "\n",
    "    def modules(self):\n",
    "        return self.__dict__[\"_modules\"].values()\n",
    "\n",
    "    def parameters(self, recurse: bool = True) -> Iterator[Parameter]:\n",
    "        for param in self.__dict__[\"_parameters\"].values():\n",
    "            yield param\n",
    "        if recurse:\n",
    "            for module in self.modules():\n",
    "                yield from module.parameters(recurse=True)\n",
    "\n",
    "    def __setattr__(self, key: str, val: Any) -> None:\n",
    "        if isinstance(val, Parameter):\n",
    "            self.__dict__.setdefault(\"_parameters\", {})[key] = val\n",
    "        elif isinstance(val, Module):\n",
    "            self.__dict__.setdefault(\"_modules\", {})[key] = val\n",
    "        else:\n",
    "            super().__setattr__(key, val)\n",
    "\n",
    "    def __getattr__(self, key: str) -> Union[Parameter, \"Module\"]:\n",
    "        if \"_parameters\" in self.__dict__ and key in self.__dict__[\"_parameters\"]:\n",
    "            return self.__dict__[\"_parameters\"][key]\n",
    "        if \"_modules\" in self.__dict__ and key in self.__dict__[\"_modules\"]:\n",
    "            return self.__dict__[\"_modules\"][key]\n",
    "        raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{key}'\")\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.weight = Parameter(np.random.randn(out_features, in_features) * np.sqrt(2. / in_features), requires_grad=True)\n",
    "        if bias:\n",
    "            self.bias = Parameter(np.zeros(out_features), requires_grad=True)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x: Parameter) -> Parameter:\n",
    "        output = x @ self.weight.T\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        return output\n",
    "\n",
    "class ReLU(Module):\n",
    "    def forward(self, x: Parameter) -> Parameter:\n",
    "        return relu(x)\n",
    "\n",
    "\n",
    "class NoGrad:\n",
    "    def __enter__(self):\n",
    "        global grad_tracking_enabled\n",
    "        self.prev_state = grad_tracking_enabled\n",
    "        grad_tracking_enabled = False\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        global grad_tracking_enabled\n",
    "        grad_tracking_enabled = self.prev_state\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "num_samples = 1000\n",
    "input_size = 784  \n",
    "num_classes = 10\n",
    "X_train = Parameter(np.random.randn(num_samples, input_size), requires_grad=False)\n",
    "y_train = Parameter(np.random.randint(0, num_classes, size=(num_samples,)), requires_grad=False)\n",
    "class SimpleNN(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = Linear(784, 128)\n",
    "        self.relu = ReLU()\n",
    "        self.fc2 = Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.relu.forward(x)\n",
    "        x = self.fc2.forward(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SimpleNN()\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 50\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "   \n",
    "    logits = model.forward(X_train)\n",
    "    loss = cross_entropy_loss(logits, y_train)\n",
    "    losses.append(loss.array)\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            param.array -= learning_rate * param.grad\n",
    "            param.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.array}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 3.101780981301624\n",
      "Epoch 2/2, Loss: 3.070237892668103\n",
      "Comparing with PyTorch:\n",
      "Forward result difference: 0.0\n",
      "a_param grad difference: 0.0\n",
      "b_param grad difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Any, Tuple, Optional, Dict, Iterator, Union\n",
    "\n",
    "#########################\n",
    "# Array Abstraction\n",
    "#########################\n",
    "\n",
    "Arr = np.ndarray  # You can change this if you have a custom array type\n",
    "\n",
    "def as_arr(data, dtype=None) -> Arr:\n",
    "    return np.array(data, dtype=dtype)\n",
    "\n",
    "def ones_like(x: Arr) -> Arr:\n",
    "    return np.ones_like(x)\n",
    "\n",
    "def zeros_like(x: Arr) -> Arr:\n",
    "    return np.zeros_like(x)\n",
    "\n",
    "def sum_arr(x: Arr, axis=None, keepdims=False) -> Arr:\n",
    "    return x.sum(axis=axis, keepdims=keepdims)\n",
    "\n",
    "def max_arr(x: Arr, axis=None, keepdims=False) -> Arr:\n",
    "    return x.max(axis=axis, keepdims=keepdims)\n",
    "\n",
    "def exp_arr(x: Arr) -> Arr:\n",
    "    return np.exp(x)\n",
    "\n",
    "def log_arr(x: Arr) -> Arr:\n",
    "    return np.log(x)\n",
    "\n",
    "def transpose_arr(x: Arr) -> Arr:\n",
    "    return x.T\n",
    "\n",
    "def reshape_arr(x: Arr, shape) -> Arr:\n",
    "    return x.reshape(shape)\n",
    "\n",
    "def matmul_arr(x: Arr, y: Arr) -> Arr:\n",
    "    return x @ y\n",
    "\n",
    "def abs_arr(x: Arr) -> Arr:\n",
    "    return np.abs(x)\n",
    "\n",
    "def broadcast_to(x: Arr, shape) -> Arr:\n",
    "    return np.broadcast_to(x, shape)\n",
    "\n",
    "def randn(*shape) -> Arr:\n",
    "    return np.random.randn(*shape)\n",
    "\n",
    "def randint(low, high, size) -> Arr:\n",
    "    return np.random.randint(low, high, size=size)\n",
    "\n",
    "grad_tracking_enabled = True\n",
    "\n",
    "#########################\n",
    "# Wrap forward functions\n",
    "#########################\n",
    "\n",
    "def wrap_forward_function(func):\n",
    "    def new_function(*args, **kwargs):\n",
    "        arguments = tuple([a for a in args])\n",
    "        result = func(*args, **kwargs)\n",
    "        requires_grad = grad_tracking_enabled and any([isinstance(a, Parameter) and a.requires_grad for a in args])\n",
    "        if requires_grad:\n",
    "            result.parents = arguments\n",
    "            result.func = new_function  \n",
    "            result.kwargs = kwargs\n",
    "            result.requires_grad = True\n",
    "        return result\n",
    "    new_function.__name__ = func.__name__\n",
    "    return new_function\n",
    "\n",
    "#########################\n",
    "# Parameter Class\n",
    "#########################\n",
    "\n",
    "class Parameter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        array: Optional[Arr] = None,\n",
    "        requires_grad: bool = False,\n",
    "        parents: Optional[Tuple['Parameter', ...]] = None,\n",
    "        func: Optional[Callable] = None,\n",
    "        kwargs: Optional[dict] = None,\n",
    "    ):\n",
    "        self.array = array if array is not None else as_arr(0.0)\n",
    "        self.grad: Optional[Arr] = None\n",
    "        self.requires_grad = requires_grad\n",
    "        self.parents = parents if parents is not None else ()\n",
    "        self.func = func\n",
    "        self.kwargs = kwargs if kwargs is not None else {}\n",
    "\n",
    "    def __add__(self, other: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(other, Parameter):\n",
    "            other = Parameter(as_arr(other), requires_grad=False)\n",
    "        return add(self, other)\n",
    "\n",
    "    def __radd__(self, other: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(other, Parameter):\n",
    "            other = Parameter(as_arr(other), requires_grad=False)\n",
    "        return add(other, self)\n",
    "\n",
    "    def __sub__(self, other: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(other, Parameter):\n",
    "            other = Parameter(as_arr(other), requires_grad=False)\n",
    "        return subtract(self, other)\n",
    "\n",
    "    def __rsub__(self, other: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(other, Parameter):\n",
    "            other = Parameter(as_arr(other), requires_grad=False)\n",
    "        return subtract(other, self)\n",
    "\n",
    "    def __mul__(self, other: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(other, Parameter):\n",
    "            other = Parameter(as_arr(other), requires_grad=False)\n",
    "        return multiply(self, other)\n",
    "\n",
    "    def __rmul__(self, other: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(other, Parameter):\n",
    "            other = Parameter(as_arr(other), requires_grad=False)\n",
    "        return multiply(other, self)\n",
    "\n",
    "    def __truediv__(self, other: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(other, Parameter):\n",
    "            other = Parameter(as_arr(other), requires_grad=False)\n",
    "        return divide_op(self, other)\n",
    "\n",
    "    def __rtruediv__(self, other: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(other, Parameter):\n",
    "            other = Parameter(as_arr(other), requires_grad=False)\n",
    "        return divide_op(other, self)\n",
    "\n",
    "    def __pow__(self, power: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(power, Parameter):\n",
    "            power = Parameter(as_arr(power), requires_grad=False)\n",
    "        return power_func(self, power)\n",
    "\n",
    "    def __neg__(self) -> 'Parameter':\n",
    "        return negate(self)\n",
    "\n",
    "    def __matmul__(self, other: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(other, Parameter):\n",
    "            raise ValueError(\"Cannot perform matmul with non-Parameter type\")\n",
    "        return matmul(self, other)\n",
    "\n",
    "    def __rmatmul__(self, other: Union['Parameter', float, int]) -> 'Parameter':\n",
    "        if not isinstance(other, Parameter):\n",
    "            raise ValueError(\"Cannot perform matmul with non-Parameter type\")\n",
    "        return matmul(other, self)\n",
    "\n",
    "    def exp(self) -> 'Parameter':\n",
    "        return exp(self)\n",
    "\n",
    "    def log(self) -> 'Parameter':\n",
    "        return log(self)\n",
    "\n",
    "    def sin(self) -> 'Parameter':\n",
    "        return sin(self)\n",
    "\n",
    "    def cos(self) -> 'Parameter':\n",
    "        return cos(self)\n",
    "\n",
    "    def tanh(self) -> 'Parameter':\n",
    "        return tanh(self)\n",
    "\n",
    "    def sigmoid(self) -> 'Parameter':\n",
    "        return sigmoid(self)\n",
    "\n",
    "    def relu(self) -> 'Parameter':\n",
    "        return relu(self)\n",
    "\n",
    "    def sqrt(self) -> 'Parameter':\n",
    "        return sqrt(self)\n",
    "\n",
    "    def abs(self) -> 'Parameter':\n",
    "        return abs_func(self)\n",
    "\n",
    "    def reshape(self, *shape) -> 'Parameter':\n",
    "        return reshape(self, shape)\n",
    "\n",
    "    def sum(self, axis=None, keepdims=False) -> 'Parameter':\n",
    "        return sum_func(self, axis=axis, keepdims=keepdims)\n",
    "\n",
    "    @property\n",
    "    def T(self) -> 'Parameter':\n",
    "        return transpose(self)\n",
    "\n",
    "    def backward(self, grad: Optional[Arr] = None):\n",
    "        if not self.requires_grad:\n",
    "            return\n",
    "        if grad is None:\n",
    "            grad = ones_like(self.array)\n",
    "        if self.grad is None:\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad\n",
    "        topo_order = self._topological_sort()\n",
    "        for tensor in reversed(topo_order):\n",
    "            if tensor.func is None:\n",
    "                continue\n",
    "            for idx, parent in enumerate(tensor.parents):\n",
    "                backward_func = lookup.get_backward_function(tensor.func, idx)\n",
    "                parent_grad = backward_func(*tensor.parents, tensor.grad, **tensor.kwargs)\n",
    "                if parent.requires_grad and parent_grad is not None:\n",
    "                    if parent.grad is None:\n",
    "                        parent.grad = parent_grad\n",
    "                    else:\n",
    "                        parent.grad += parent_grad\n",
    "\n",
    "    def _topological_sort(self) -> list:\n",
    "        visited = set()\n",
    "        topo_order = []\n",
    "        def dfs(tensor: 'Parameter'):\n",
    "            if tensor in visited:\n",
    "                return\n",
    "            visited.add(tensor)\n",
    "            for parent in tensor.parents:\n",
    "                dfs(parent)\n",
    "            topo_order.append(tensor)\n",
    "        dfs(self)\n",
    "        return topo_order\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.grad = None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Parameter(shape={self.array.shape}, requires_grad={self.requires_grad})\"\n",
    "\n",
    "\n",
    "\n",
    "class BackwardLookupTable:\n",
    "    '''\n",
    "    A backpropagation helper lookup table\n",
    "    Is a dictionary from the fucntion to the dictionary of positon - funciton pairs\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.table: Dict[Callable, Dict[int, Callable]] = defaultdict(dict)\n",
    "\n",
    "    def add_element(self, forward_function: Callable, position: int, backward_func: Callable):\n",
    "        self.table[forward_function][position] = backward_func\n",
    "\n",
    "    def get_backward_function(self, forward_function: Callable, position: int) -> Callable:\n",
    "        if forward_function not in self.table or position not in self.table[forward_function]:\n",
    "            raise KeyError(f\"No backward function found for {forward_function} at position {position}\")\n",
    "        return self.table[forward_function][position]\n",
    "\n",
    "lookup = BackwardLookupTable()\n",
    "\n",
    "\n",
    "\n",
    "def _broadcast_backward(grad_out, target_shape):\n",
    "    grad = grad_out\n",
    "    while len(grad.shape) > len(target_shape):\n",
    "        grad = sum_arr(grad, axis=0)\n",
    "    for axis, size in enumerate(target_shape):\n",
    "        if size == 1:\n",
    "            grad = sum_arr(grad, axis=axis, keepdims=True)\n",
    "    return grad\n",
    "\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def add(x, y):\n",
    "    return Parameter(array=x.array + y.array)\n",
    "\n",
    "def add_back0(x, y, grad_out):\n",
    "    grad = _broadcast_backward(grad_out, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "def add_back1(x, y, grad_out):\n",
    "    grad = _broadcast_backward(grad_out, y.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(add, 0, add_back0)\n",
    "lookup.add_element(add, 1, add_back1)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def subtract(x, y):\n",
    "    return Parameter(array=x.array - y.array)\n",
    "\n",
    "def subtract_back0(x, y, grad_out):\n",
    "    grad = _broadcast_backward(grad_out, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "def subtract_back1(x, y, grad_out):\n",
    "    grad = -_broadcast_backward(grad_out, y.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(subtract, 0, subtract_back0)\n",
    "lookup.add_element(subtract, 1, subtract_back1)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def multiply(x, y):\n",
    "    return Parameter(array=x.array * y.array)\n",
    "\n",
    "def multiply_back0(x, y, grad_out):\n",
    "    grad = grad_out * y.array\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "def multiply_back1(x, y, grad_out):\n",
    "    grad = grad_out * x.array\n",
    "    grad = _broadcast_backward(grad, y.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(multiply, 0, multiply_back0)\n",
    "lookup.add_element(multiply, 1, multiply_back1)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def divide_op(x, y):\n",
    "    return Parameter(array=x.array / y.array)\n",
    "\n",
    "def divide_back0(x, y, grad_out):\n",
    "    grad = grad_out / y.array\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "def divide_back1(x, y, grad_out):\n",
    "    if y.requires_grad:\n",
    "        grad = -grad_out * x.array / (y.array ** 2 + 1e-12)\n",
    "        grad = _broadcast_backward(grad, y.array.shape)\n",
    "        return grad\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "lookup.add_element(divide_op, 0, divide_back0)\n",
    "lookup.add_element(divide_op, 1, divide_back1)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def power_func(x, y):\n",
    "    return Parameter(array=x.array ** y.array)\n",
    "\n",
    "def power_back0(x, y, grad_out):\n",
    "    grad = grad_out * y.array * (x.array ** (y.array - 1))\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "def power_back1(x, y, grad_out):\n",
    "    grad = grad_out * (x.array ** y.array) * log_arr(x.array + 1e-12)\n",
    "    grad = _broadcast_backward(grad, y.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(power_func, 0, power_back0)\n",
    "lookup.add_element(power_func, 1, power_back1)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def negate(x):\n",
    "    return Parameter(array=-x.array)\n",
    "\n",
    "def negate_back(x, grad_out):\n",
    "    grad = -grad_out\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(negate, 0, negate_back)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def matmul(x, y):\n",
    "    return Parameter(array=matmul_arr(x.array, y.array))\n",
    "\n",
    "def matmul_back0(x, y, grad_out):\n",
    "    grad = matmul_arr(grad_out, transpose_arr(y.array))\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "def matmul_back1(x, y, grad_out):\n",
    "    grad = matmul_arr(transpose_arr(x.array), grad_out)\n",
    "    grad = _broadcast_backward(grad, y.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(matmul, 0, matmul_back0)\n",
    "lookup.add_element(matmul, 1, matmul_back1)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def exp(x):\n",
    "    return Parameter(array=exp_arr(x.array))\n",
    "\n",
    "def exp_back(x, grad_out):\n",
    "    grad = grad_out * exp_arr(x.array)\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(exp, 0, exp_back)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def log(x):\n",
    "    return Parameter(array=log_arr(x.array + 1e-12))\n",
    "\n",
    "def log_back(x, grad_out):\n",
    "    grad = grad_out / (x.array + 1e-12)\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(log, 0, log_back)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def sum_func(x, axis=None, keepdims=False):\n",
    "    return Parameter(array=sum_arr(x.array, axis=axis, keepdims=keepdims))\n",
    "\n",
    "def sum_back(x, grad_out, axis=None, keepdims=False):\n",
    "    grad = broadcast_to(grad_out, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(sum_func, 0, sum_back)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def max_func(x, axis=None, keepdims=False):\n",
    "    return Parameter(array=max_arr(x.array, axis=axis, keepdims=keepdims))\n",
    "\n",
    "def max_back(x, grad_out, axis=None, keepdims=False):\n",
    "    grad = zeros_like(x.array)\n",
    "    max_vals = max_arr(x.array, axis=axis, keepdims=True)\n",
    "    mask = (x.array == max_vals)\n",
    "    grad += mask * grad_out\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(max_func, 0, max_back)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def transpose(x):\n",
    "    return Parameter(array=transpose_arr(x.array))\n",
    "\n",
    "def transpose_back(x, grad_out):\n",
    "    return grad_out.T\n",
    "\n",
    "lookup.add_element(transpose, 0, transpose_back)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def relu(x):\n",
    "    return Parameter(array=np.maximum(0, x.array))\n",
    "\n",
    "def relu_back(x, grad_out):\n",
    "    grad = grad_out.copy()\n",
    "    grad[x.array <= 0] = 0\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(relu, 0, relu_back)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def abs_func(x):\n",
    "    return Parameter(array=abs_arr(x.array))\n",
    "\n",
    "def abs_back(x, grad_out):\n",
    "    grad = grad_out * np.sign(x.array)\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(abs_func, 0, abs_back)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def sqrt(x):\n",
    "    return Parameter(array=np.sqrt(x.array + 1e-12))\n",
    "\n",
    "def sqrt_back(x, grad_out):\n",
    "    grad = grad_out * (1.0 / (2.0 * np.sqrt(x.array + 1e-12)))\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(sqrt, 0, sqrt_back)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def sin(x):\n",
    "    return Parameter(array=np.sin(x.array))\n",
    "\n",
    "def sin_back(x, grad_out):\n",
    "    grad = grad_out * np.cos(x.array)\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(sin, 0, sin_back)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def cos(x):\n",
    "    return Parameter(array=np.cos(x.array))\n",
    "\n",
    "def cos_back(x, grad_out):\n",
    "    grad = -grad_out * np.sin(x.array)\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(cos, 0, cos_back)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def tanh(x):\n",
    "    val = np.tanh(x.array)\n",
    "    return Parameter(array=val)\n",
    "\n",
    "def tanh_back(x, grad_out):\n",
    "    val = np.tanh(x.array)\n",
    "    grad = grad_out * (1 - val**2)\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(tanh, 0, tanh_back)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def sigmoid(x):\n",
    "    val = 1 / (1 + np.exp(-x.array))\n",
    "    return Parameter(array=val)\n",
    "\n",
    "def sigmoid_back(x, grad_out):\n",
    "    val = 1 / (1 + np.exp(-x.array))\n",
    "    grad = grad_out * val * (1 - val)\n",
    "    grad = _broadcast_backward(grad, x.array.shape)\n",
    "    return grad\n",
    "\n",
    "lookup.add_element(sigmoid, 0, sigmoid_back)\n",
    "\n",
    "\n",
    "@wrap_forward_function\n",
    "def reshape(x, shape):\n",
    "    return Parameter(array=reshape_arr(x.array, shape))\n",
    "\n",
    "def reshape_back(x, grad_out, shape):\n",
    "    return reshape_arr(grad_out, x.array.shape)\n",
    "\n",
    "lookup.add_element(reshape, 0, reshape_back)\n",
    "\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    x_max = max_func(x, axis=1, keepdims=True)\n",
    "    exp_x = exp(x - x_max)\n",
    "    sum_exp_x = sum_func(exp_x, axis=1, keepdims=True)\n",
    "    return exp_x / sum_exp_x\n",
    "\n",
    "def one_hot(targets, num_classes):\n",
    "    targets_array = targets.array.astype(int)\n",
    "    one_hot_array = np.zeros((len(targets_array), num_classes))\n",
    "    one_hot_array[np.arange(len(targets_array)), targets_array] = 1\n",
    "    return Parameter(one_hot_array, requires_grad=False)\n",
    "\n",
    "def cross_entropy_loss(predictions, targets):\n",
    "    probs = softmax(predictions)\n",
    "    N = predictions.array.shape[0]\n",
    "    target_probs = one_hot(targets, num_classes=predictions.array.shape[1])\n",
    "    epsilon = 1e-12\n",
    "    log_probs = log(probs + epsilon)\n",
    "    loss = -sum_func(target_probs * log_probs) / N\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "class Module:\n",
    "    def __init__(self):\n",
    "        self._modules = {}\n",
    "        self._parameters = {}\n",
    "\n",
    "    def modules(self):\n",
    "        return self.__dict__[\"_modules\"].values()\n",
    "\n",
    "    def parameters(self, recurse: bool = True) -> Iterator[Parameter]:\n",
    "        for param in self.__dict__[\"_parameters\"].values():\n",
    "            yield param\n",
    "        if recurse:\n",
    "            for module in self.modules():\n",
    "                yield from module.parameters(recurse=True)\n",
    "\n",
    "    def __setattr__(self, key: str, val: Any) -> None:\n",
    "        if isinstance(val, Parameter):\n",
    "            self.__dict__.setdefault(\"_parameters\", {})[key] = val\n",
    "        elif isinstance(val, Module):\n",
    "            self.__dict__.setdefault(\"_modules\", {})[key] = val\n",
    "        else:\n",
    "            super().__setattr__(key, val)\n",
    "\n",
    "    def __getattr__(self, key: str) -> Union[Parameter, \"Module\"]:\n",
    "        if \"_parameters\" in self.__dict__ and key in self.__dict__[\"_parameters\"]:\n",
    "            return self.__dict__[\"_parameters\"][key]\n",
    "        if \"_modules\" in self.__dict__ and key in self.__dict__[\"_modules\"]:\n",
    "            return self.__dict__[\"_modules\"][key]\n",
    "        raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{key}'\")\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.weight = Parameter(randn(out_features, in_features) * np.sqrt(2. / in_features), requires_grad=True)\n",
    "        if bias:\n",
    "            self.bias = Parameter(as_arr(np.zeros(out_features)), requires_grad=True)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x: Parameter) -> Parameter:\n",
    "        output = x @ self.weight.T\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        return output\n",
    "\n",
    "class ReLU(Module):\n",
    "    def forward(self, x: Parameter) -> Parameter:\n",
    "        return relu(x)\n",
    "\n",
    "class NoGrad:\n",
    "    def __enter__(self):\n",
    "        global grad_tracking_enabled\n",
    "        self.prev_state = grad_tracking_enabled\n",
    "        grad_tracking_enabled = False\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        global grad_tracking_enabled\n",
    "        grad_tracking_enabled = self.prev_state\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "num_samples = 1000\n",
    "input_size = 784  \n",
    "num_classes = 10\n",
    "X_train = Parameter(randn(num_samples, input_size), requires_grad=False)\n",
    "y_train = Parameter(randint(0, num_classes, size=(num_samples,)), requires_grad=False)\n",
    "\n",
    "class SimpleNN(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = Linear(784, 128)\n",
    "        self.relu = ReLU()\n",
    "        self.fc2 = Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.relu.forward(x)\n",
    "        x = self.fc2.forward(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN()\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 2  # reduced for brevity\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    logits = model.forward(X_train)\n",
    "    loss = cross_entropy_loss(logits, y_train)\n",
    "    losses.append(loss.array)\n",
    "    \n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            param.array -= learning_rate * param.grad\n",
    "            param.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.array}\")\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    \n",
    "    \n",
    "    a_np = as_arr([[1.0, 2.0],[3.0,4.0]])\n",
    "    b_np = as_arr([[5.0, 6.0],[7.0,8.0]])\n",
    "    \n",
    "    a_param = Parameter(a_np, requires_grad=True)\n",
    "    b_param = Parameter(b_np, requires_grad=True)\n",
    "    \n",
    "   \n",
    "    out_param = a_param * b_param + a_param @ b_param\n",
    "    out_param.backward()\n",
    "\n",
    "  \n",
    "    a_torch = torch.tensor(a_np, requires_grad=True)\n",
    "    b_torch = torch.tensor(b_np, requires_grad=True)\n",
    "    out_torch = a_torch * b_torch + a_torch.matmul(b_torch)\n",
    "    out_torch.backward(torch.ones_like(out_torch))\n",
    "\n",
    "\n",
    "    print(\"Comparing with PyTorch:\")\n",
    "    print(\"Forward result difference:\", np.abs((out_param.array - out_torch.detach().numpy())).sum())\n",
    "    print(\"a_param grad difference:\", np.abs((a_param.grad - a_torch.grad.numpy())).sum())\n",
    "    print(\"b_param grad difference:\", np.abs((b_param.grad - b_torch.grad.numpy())).sum())\n",
    "\n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed. Skipping comparison test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp312-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Downloading torch-2.5.1-cp312-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, setuptools, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.16.1 fsspec-2024.12.0 jinja2-3.1.4 mpmath-1.3.0 networkx-3.4.2 setuptools-75.6.0 sympy-1.13.1 torch-2.5.1 typing-extensions-4.12.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
