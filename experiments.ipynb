{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "from typing import Callable, Any, DefaultDict, Tuple, Optional\n",
    "from typing import Any, Callable, Iterable, Iterator, TypeAlias, DefaultDict\n",
    "from typing import Union, Iterator, Any\n",
    "grad_tracking_enabled = True\n",
    "Arr : np.ndarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_forward_function(func): #function should  be from tensors to tensors \n",
    "    def new_function(*args, **kwargs):\n",
    "        arguments = tuple([a for a in args]) #we expect args to be tensors \n",
    "        result = func(*args, **kwargs)\n",
    "        requires_grad = grad_tracking_enabled and any([a.requires_grad for a in args])\n",
    "        if requires_grad:\n",
    "            result.parents = arguments\n",
    "        return result\n",
    "    return new_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        array: Optional[np.ndarray] = None,\n",
    "        requires_grad: bool = False,\n",
    "        parents: Optional[Tuple['Parameter', ...]] = None,\n",
    "        func: Optional[Callable] = None,\n",
    "        kwargs: Optional[dict] = None,\n",
    "    ):\n",
    "        self.array = array if array is not None else np.array(0.0)\n",
    "        self.grad: Optional[np.ndarray] = None\n",
    "        self.requires_grad = requires_grad\n",
    "        self.parents = parents if parents is not None else ()\n",
    "        self.func = func\n",
    "        self.kwargs = kwargs if kwargs is not None else {}\n",
    "    def __add__(self, other: 'Parameter') -> 'Parameter':\n",
    "        return sum_(self, other)\n",
    "    def __mul__(self, other: 'Parameter') -> 'Parameter':\n",
    "        return multiply(self, other)\n",
    "    def __rmul__(self, other: 'Parameter') -> 'Parameter':\n",
    "        return multiply(other, self)\n",
    "    def __matmul__(self, other: 'Parameter') -> 'Parameter':\n",
    "        return matmul(self, other)\n",
    "    def __rmatmul__(self, other: 'Parameter') -> 'Parameter':\n",
    "        return matmul(other, self)\n",
    "    @property\n",
    "    def T(self) -> 'Parameter':\n",
    "        return Parameter(array=self.array.T, requires_grad=self.requires_grad)\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Parameter(shape={self.array.shape}, requires_grad={self.requires_grad})\"\n",
    "    def backward(self, grad: Optional[np.ndarray] = None):\n",
    "        if not self.requires_grad:\n",
    "            return\n",
    "        if grad is None:\n",
    "            grad = np.ones_like(self.array)\n",
    "        self.grad = grad if self.grad is None else self.grad + grad\n",
    "        topo_order = self._topological_sort()\n",
    "        for tensor in reversed(topo_order):\n",
    "            if tensor.func is None:\n",
    "                continue  \n",
    "            for idx, parent in enumerate(tensor.parents):\n",
    "                backward_func = lookup.get_backward_function(tensor.func, idx)\n",
    "                parent_grad = backward_func(*tensor.parents, tensor.grad)\n",
    "\n",
    "                if parent.requires_grad:\n",
    "                    if parent.grad is None:\n",
    "                        parent.grad = parent_grad\n",
    "                    else:\n",
    "                        parent.grad += parent_grad\n",
    "\n",
    "    def _topological_sort(self) -> list:\n",
    "        visited = set()\n",
    "        topo_order = []\n",
    "        def dfs(tensor: 'Parameter'):\n",
    "            if tensor in visited:\n",
    "                return\n",
    "            visited.add(tensor)\n",
    "            for parent in tensor.parents:\n",
    "                dfs(parent)\n",
    "            topo_order.append(tensor)\n",
    "        dfs(self)\n",
    "        return topo_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wrap_forward_function\n",
    "def multiply(x, y):\n",
    "    return Tensor(x.array * y.array, parents = (x, y))\n",
    "\n",
    "@wrap_forward_function\n",
    "def matmul(x, y):\n",
    "    return Tensor(x.array @ y.array, parents = (x, y))\n",
    "\n",
    "@wrap_forward_function\n",
    "def sum_(x, y):\n",
    "    return Tensor(x.array + y.array, parents=(x, y), requires_grad=True)\n",
    "\n",
    "@wrap_forward_function\n",
    "def log(x):\n",
    "    return Tensor(np.log(x.array))\n",
    "@wrap_forward_function\n",
    "def eq(x, y):\n",
    "    return Tensor(np.equal(x.array, y.array))\n",
    "@wrap_forward_function\n",
    "def sum(x, dim=None, keepdim=False):\n",
    "    return Tensor(np.sum(x.array, axis=dim, keepdims=keepdim), parents=(x,), kwargs={'dim': dim, 'keepdim': keepdim}, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackwardLookupTable:\n",
    "    def __init__(self):\n",
    "        self.table: DefaultDict[Callable, Dict[int, Callable]] = defaultdict(dict)\n",
    "\n",
    "    def add_element(self, forward_function: Callable, position: int, backward_func: Callable):\n",
    "        self.table[forward_function][position] = backward_func\n",
    "\n",
    "    def get_backward_function(self, forward_function: Callable, position: int) -> Callable:\n",
    "        return self.table[forward_function][position]\n",
    "\n",
    "   \n",
    "\n",
    "def multiply_back0(argument0, argument1, grad_out):\n",
    "    return  argument1.array * grad_out\n",
    "def multiply_back1(argument0, argument1, grad_out):\n",
    "    return  argument0.array * grad_out\n",
    "def matmul_back0(argument0, argument1, grad_out): \n",
    "    return grad_out @ argument1.T.array\n",
    "def matmul_back1(argument0, argument1, grad_out): \n",
    "    return  argument0.T.array @ grad_out \n",
    "def sum_back0(argument0, argument1, grad_out):\n",
    "    return grad_out\n",
    "def sum_back1(argument0, argument1, grad_out):\n",
    "    return grad_out\n",
    "def sum_back(argument, grad_out):\n",
    "    dim = argument.kwargs.get('dim', None)\n",
    "    keepdim = argument.kwargs.get('keepdim', False)\n",
    "    original_shape = argument.array.shape\n",
    "    grad_shape = grad_out.shape\n",
    "    if dim is None:\n",
    "        grad = np.full(original_shape, grad_out)\n",
    "    else:\n",
    "        if isinstance(dim, int):\n",
    "            dim = (dim,)\n",
    "        else:\n",
    "            dim = tuple(dim)\n",
    "        if not keepdim:\n",
    "            grad_out = np.expand_dims(grad_out, axis=dim)\n",
    "        grad = np.ones(original_shape) * grad_out\n",
    "    return grad\n",
    "\n",
    "\n",
    "\n",
    "lookup = BackwardLookupTable()\n",
    "lookup.add_element(multiply, 0, multiply_back0)\n",
    "lookup.add_element(multiply, 1, multiply_back1)\n",
    "lookup.add_element(matmul, 0, matmul_back0)\n",
    "lookup.add_element(matmul, 1, matmul_back1)\n",
    "lookup.add_element(sum_, 0, sum_back0)\n",
    "lookup.add_element(sum_, 1, sum_back1)\n",
    "lookup.add_element(sum, 0, sum_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    _modules: dict[str, \"Module\"]\n",
    "    _parameters: dict[str, Parameter]\n",
    "\n",
    "    def __init__(self):\n",
    "        self._modules = {}\n",
    "        self._parameters = {}\n",
    "    \n",
    "    def modules(self):\n",
    "        '''Return the direct child modules of this module.'''\n",
    "        return self.__dict__[\"_modules\"].values()\n",
    "    \n",
    "    def parameters(self, recurse: bool = True) -> Iterator[Parameter]:\n",
    "        '''\n",
    "        Return an iterator over Module parameters.\n",
    "        recurse: if True, the iterator includes parameters of submodules, recursively.\n",
    "        '''\n",
    "        parameters_list = list(self.__dict__[\"_parameters\"].values())\n",
    "        if recurse:\n",
    "            for mod in self.modules():\n",
    "                parameters_list.extend(list(mod.parameters(recurse=True)))\n",
    "        return iter(parameters_list)\n",
    "    \n",
    "    def __setattr__(self, key: str, val: Any) -> None:\n",
    "        '''\n",
    "        If val is a Parameter or Module, store it in the appropriate _parameters or _modules dict.\n",
    "        Otherwise, call __setattr__ from the superclass.\n",
    "        '''\n",
    "        if isinstance(val, Parameter):\n",
    "            self.__dict__[\"_parameters\"][key] = val\n",
    "        elif isinstance(val, Module):\n",
    "            self.__dict__[\"_modules\"][key] = val\n",
    "        else:\n",
    "            super().__setattr__(key, val)\n",
    "    \n",
    "    def __getattr__(self, key: str) -> Union[Parameter, \"Module\"]:  # Changed this line\n",
    "        '''\n",
    "        If key is in _parameters or _modules, return the corresponding value.\n",
    "        Otherwise, raise KeyError.\n",
    "        '''\n",
    "        if key in self.__dict__[\"_parameters\"]:\n",
    "            return self.__dict__[\"_parameters\"][key]\n",
    "        if key in self.__dict__[\"_modules\"]:\n",
    "            return self.__dict__[\"_modules\"][key]\n",
    "        raise KeyError(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
