{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "from typing import Callable, Any, DefaultDict, Tuple, Optional\n",
    "\n",
    "# ----------------------------\n",
    "# Backward Lookup Table\n",
    "# ----------------------------\n",
    "class BackwardLookupTable:\n",
    "    def __init__(self):\n",
    "        # Maps a forward function to a dictionary that maps input positions to backward functions\n",
    "        self.table: DefaultDict[Callable, Dict[int, Callable]] = defaultdict(dict)\n",
    "\n",
    "    def add_element(self, forward_function: Callable, position: int, backward_func: Callable):\n",
    "        \"\"\"\n",
    "        Registers a backward function for a given forward function and input position.\n",
    "        \"\"\"\n",
    "        self.table[forward_function][position] = backward_func\n",
    "\n",
    "    def get_backward_function(self, forward_function: Callable, position: int) -> Callable:\n",
    "        \"\"\"\n",
    "        Retrieves the backward function for a given forward function and input position.\n",
    "        \"\"\"\n",
    "        return self.table[forward_function][position]\n",
    "\n",
    "# Initialize the backward lookup table\n",
    "lookup = BackwardLookupTable()\n",
    "\n",
    "# ----------------------------\n",
    "# Backward Functions\n",
    "# ----------------------------\n",
    "def multiply_back0(arg0: 'Parameter', arg1: 'Parameter', grad_out: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Backward function for the first input of element-wise multiplication.\n",
    "    Gradient w.r. to arg0 is arg1.array * grad_out\n",
    "    \"\"\"\n",
    "    return arg1.array * grad_out\n",
    "\n",
    "def multiply_back1(arg0: 'Parameter', arg1: 'Parameter', grad_out: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Backward function for the second input of element-wise multiplication.\n",
    "    Gradient w.r. to arg1 is arg0.array * grad_out\n",
    "    \"\"\"\n",
    "    return arg0.array * grad_out\n",
    "\n",
    "def matmul_back0(arg0: 'Parameter', arg1: 'Parameter', grad_out: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Backward function for the first input of matrix multiplication.\n",
    "    Gradient w.r. to arg0 is grad_out @ arg1.array.T\n",
    "    \"\"\"\n",
    "    return grad_out @ arg1.array.T\n",
    "\n",
    "def matmul_back1(arg0: 'Parameter', arg1: 'Parameter', grad_out: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Backward function for the second input of matrix multiplication.\n",
    "    Gradient w.r. to arg1 is arg0.array.T @ grad_out\n",
    "    \"\"\"\n",
    "    return arg0.array.T @ grad_out\n",
    "\n",
    "def sum_back(arg: 'Parameter', grad_out: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Backward function for the sum operation over a single tensor.\n",
    "    It broadcasts grad_out to the shape of the original tensor.\n",
    "    \"\"\"\n",
    "    dim = arg.kwargs.get('dim', None)\n",
    "    keepdim = arg.kwargs.get('keepdim', False)\n",
    "    \n",
    "    original_shape = arg.parents[0].array.shape  # Assuming single parent\n",
    "    if dim is None:\n",
    "        # Sum over all elements; grad_out is a scalar\n",
    "        grad = np.full(original_shape, grad_out)\n",
    "    else:\n",
    "        if isinstance(dim, int):\n",
    "            dim = (dim,)\n",
    "        else:\n",
    "            dim = tuple(dim)\n",
    "        \n",
    "        if not keepdim:\n",
    "            # Expand dimensions for broadcasting\n",
    "            grad_out = grad_out\n",
    "            for d in sorted(dim):\n",
    "                grad_out = np.expand_dims(grad_out, axis=d)\n",
    "        \n",
    "        # Broadcast the gradient to the original shape\n",
    "        grad = np.ones(original_shape) * grad_out\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def sum_back0(arg0: 'Parameter', arg1: 'Parameter', grad_out: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Backward function for the first input of addition.\n",
    "    Gradient w.r. to arg0 is grad_out.\n",
    "    \"\"\"\n",
    "    return grad_out\n",
    "\n",
    "def sum_back1(arg0: 'Parameter', arg1: 'Parameter', grad_out: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Backward function for the second input of addition.\n",
    "    Gradient w.r. to arg1 is grad_out.\n",
    "    \"\"\"\n",
    "    return grad_out\n",
    "\n",
    "# ----------------------------\n",
    "# Forward Operations with Decorator\n",
    "# ----------------------------\n",
    "def wrap_forward_function(func: Callable) -> Callable:\n",
    "    \"\"\"\n",
    "    Decorator to wrap forward functions to handle gradient tracking.\n",
    "    Sets the parents of the result tensor if gradients are required.\n",
    "    \"\"\"\n",
    "    def new_function(*args: 'Parameter', **kwargs: Any) -> 'Parameter':\n",
    "        # Execute the forward function\n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        # Determine if gradients need to be tracked\n",
    "        requires_grad = any([arg.requires_grad for arg in args])\n",
    "        if requires_grad:\n",
    "            result.parents = args\n",
    "            result.func = func\n",
    "            result.kwargs = kwargs\n",
    "        return result\n",
    "    return new_function\n",
    "\n",
    "@wrap_forward_function\n",
    "def multiply(x: 'Parameter', y: 'Parameter') -> 'Parameter':\n",
    "    \"\"\"\n",
    "    Element-wise multiplication of two tensors.\n",
    "    \"\"\"\n",
    "    return Parameter(array=x.array * y.array, requires_grad=True)\n",
    "\n",
    "@wrap_forward_function\n",
    "def matmul(x: 'Parameter', y: 'Parameter') -> 'Parameter':\n",
    "    \"\"\"\n",
    "    Matrix multiplication of two tensors.\n",
    "    \"\"\"\n",
    "    return Parameter(array=x.array @ y.array, requires_grad=True)\n",
    "\n",
    "@wrap_forward_function\n",
    "def sum_(x: 'Parameter', y: 'Parameter') -> 'Parameter':\n",
    "    \"\"\"\n",
    "    Element-wise addition of two tensors.\n",
    "    \"\"\"\n",
    "    return Parameter(array=x.array + y.array, requires_grad=True)\n",
    "\n",
    "@wrap_forward_function\n",
    "def sum(x: 'Parameter', dim: Optional[int] = None, keepdim: bool = False) -> 'Parameter':\n",
    "    \"\"\"\n",
    "    Sum of a tensor along specified dimensions.\n",
    "    \"\"\"\n",
    "    return Parameter(array=np.sum(x.array, axis=dim, keepdims=keepdim), requires_grad=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Register Backward Functions\n",
    "# ----------------------------\n",
    "# Register backward functions for element-wise multiplication\n",
    "lookup.add_element(multiply, 0, multiply_back0)\n",
    "lookup.add_element(multiply, 1, multiply_back1)\n",
    "\n",
    "# Register backward functions for matrix multiplication\n",
    "lookup.add_element(matmul, 0, matmul_back0)\n",
    "lookup.add_element(matmul, 1, matmul_back1)\n",
    "\n",
    "# Register backward functions for addition of two tensors\n",
    "lookup.add_element(sum_, 0, sum_back0)\n",
    "lookup.add_element(sum_, 1, sum_back1)\n",
    "\n",
    "# Register backward function for sum over a single tensor\n",
    "lookup.add_element(sum, 0, sum_back)\n",
    "\n",
    "# ----------------------------\n",
    "# Parameter Class\n",
    "# ----------------------------\n",
    "class Parameter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        array: Optional[np.ndarray] = None,\n",
    "        requires_grad: bool = False,\n",
    "        parents: Optional[Tuple['Parameter', ...]] = None,\n",
    "        func: Optional[Callable] = None,\n",
    "        kwargs: Optional[dict] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a Parameter.\n",
    "        \n",
    "        :param array: The underlying NumPy array.\n",
    "        :param requires_grad: Flag indicating whether to track gradients.\n",
    "        :param parents: Parent tensors involved in creating this tensor.\n",
    "        :param func: The forward function that created this tensor.\n",
    "        :param kwargs: Additional keyword arguments passed to the forward function.\n",
    "        \"\"\"\n",
    "        self.array = array if array is not None else np.array(0.0)\n",
    "        self.grad: Optional[np.ndarray] = None\n",
    "        self.requires_grad = requires_grad\n",
    "        self.parents = parents if parents is not None else ()\n",
    "        self.func = func\n",
    "        self.kwargs = kwargs if kwargs is not None else {}\n",
    "    \n",
    "    def __add__(self, other: 'Parameter') -> 'Parameter':\n",
    "        return sum_(self, other)\n",
    "\n",
    "    def __mul__(self, other: 'Parameter') -> 'Parameter':\n",
    "        return multiply(self, other)\n",
    "\n",
    "    def __rmul__(self, other: 'Parameter') -> 'Parameter':\n",
    "        return multiply(other, self)\n",
    "\n",
    "    def __matmul__(self, other: 'Parameter') -> 'Parameter':\n",
    "        return matmul(self, other)\n",
    "\n",
    "    def __rmatmul__(self, other: 'Parameter') -> 'Parameter':\n",
    "        return matmul(other, self)\n",
    "\n",
    "    @property\n",
    "    def T(self) -> 'Parameter':\n",
    "        \"\"\"\n",
    "        Transpose of the tensor.\n",
    "        \"\"\"\n",
    "        return Parameter(array=self.array.T, requires_grad=self.requires_grad)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Parameter(shape={self.array.shape}, requires_grad={self.requires_grad})\"\n",
    "\n",
    "    def backward(self, grad: Optional[np.ndarray] = None):\n",
    "        \"\"\"\n",
    "        Initiates the backward pass to compute gradients.\n",
    "        \n",
    "        :param grad: Gradient of the loss with respect to this tensor.\n",
    "                     If None, it is assumed to be a scalar tensor with grad 1.\n",
    "        \"\"\"\n",
    "        if not self.requires_grad:\n",
    "            return\n",
    "\n",
    "        if grad is None:\n",
    "            # If grad is not provided, assume this tensor is a scalar\n",
    "            grad = np.ones_like(self.array)\n",
    "        \n",
    "        # Initialize the gradient\n",
    "        self.grad = grad if self.grad is None else self.grad + grad\n",
    "\n",
    "        # Perform topological sort\n",
    "        topo_order = self._topological_sort()\n",
    "\n",
    "        # Traverse the graph in reverse topological order\n",
    "        for tensor in reversed(topo_order):\n",
    "            if tensor.func is None:\n",
    "                continue  # Skip leaf tensors\n",
    "\n",
    "            # Iterate over all parents and compute their gradients\n",
    "            for idx, parent in enumerate(tensor.parents):\n",
    "                backward_func = lookup.get_backward_function(tensor.func, idx)\n",
    "                parent_grad = backward_func(*tensor.parents, tensor.grad)\n",
    "\n",
    "                if parent.requires_grad:\n",
    "                    if parent.grad is None:\n",
    "                        parent.grad = parent_grad\n",
    "                    else:\n",
    "                        parent.grad += parent_grad\n",
    "\n",
    "    def _topological_sort(self) -> list:\n",
    "        \"\"\"\n",
    "        Performs a topological sort of the computation graph.\n",
    "        \n",
    "        :return: List of tensors in topologically sorted order.\n",
    "        \"\"\"\n",
    "        visited = set()\n",
    "        topo_order = []\n",
    "\n",
    "        def dfs(tensor: 'Parameter'):\n",
    "            if tensor in visited:\n",
    "                return\n",
    "            visited.add(tensor)\n",
    "            for parent in tensor.parents:\n",
    "                dfs(parent)\n",
    "            topo_order.append(tensor)\n",
    "\n",
    "        dfs(self)\n",
    "        return topo_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
